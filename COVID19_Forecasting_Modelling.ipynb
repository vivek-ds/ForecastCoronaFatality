{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for predicting fatalities and confirmed cases\n",
    "\n",
    "We follow the below steps for the modelling process:\n",
    "\n",
    "* Load Required Packages and Datasets\n",
    "* Combine Train and Test Data\n",
    "* Join Government Measures Data\n",
    "* Join Distance from China\n",
    "* Join COVID Indicators\n",
    "* Prepare Data\n",
    "* Split into Train and Test Sets\n",
    "* Functions to make predictions\n",
    "* Linear Models\n",
    "    + Linear Regression\n",
    "    + Lasso Regression\n",
    "    + Ridge Regression\n",
    "* Non-Linear Models\n",
    "    + Decision Trees\n",
    "    + Random Forests\n",
    "    + Gradient Boosting\n",
    "* Choosing the Best Model for Submission\n",
    "\n",
    "Our focus in the modelling process is to model data for predicting fatalities and confirmed cases, so we won't be diving much into the inferences from each of these models, except evaluating their performance in terms of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Necessary Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import io\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import ensemble\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "from geopy.distance import distance\n",
    "from geopy import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Kaggle Files\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\", encoding= 'unicode_escape', parse_dates = ['Date'])\n",
    "test_data = pd.read_csv(\"test.csv\", encoding= 'unicode_escape', parse_dates = ['Date'])\n",
    "submission_data = pd.read_csv(\"submission.csv\", encoding= 'unicode_escape')\n",
    "\n",
    "# Loading Distance From China Data\n",
    "lat_long = pd.read_csv(\"johns-hopkins-covid-19-daily-dashboard-cases-by-country.csv\", encoding= 'unicode_escape')\n",
    "\n",
    "# Loading Government Measurement Data\n",
    "\n",
    "govt_measures_data = pd.read_csv(\"acaps-covid-19-government-measures-dataset.csv\", encoding= 'unicode_escape')\n",
    "\n",
    "# Loading Covid Indicators Data\n",
    "\n",
    "covid_indicators_data = pd.read_csv(\"inform-covid-indicators.csv\", encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Adding Indicator Columns to identify datasets\n",
    "train_data['data_set'] = 'Train'\n",
    "test_data['data_set'] = 'Test'\n",
    "\n",
    "#Convert Target columns into log scale\n",
    "train_data['ConfirmedCases'] = np.log(train_data['ConfirmedCases']+1)\n",
    "train_data['Fatalities'] = np.log(train_data['Fatalities']+1)\n",
    "\n",
    "#Adding columns to test data set\n",
    "test_data = test_data.rename(columns={\"ForecastId\": \"Id\"})\n",
    "test_data['ConfirmedCases'] = None\n",
    "test_data['Fatalities'] = None\n",
    "\n",
    "data = pd.concat([train_data,test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days since first occurence\n",
    "\n",
    "data['days'] = data['Date']-data['Date'].min()\n",
    "data['days'] = data['days'].astype('timedelta64[D]').astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Government Measures Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Data\n",
    "\n",
    "var_req = ['country', 'measure']\n",
    "govt_measures_data = govt_measures_data[var_req]\n",
    "govt_measures_data['measure'] = govt_measures_data['measure'].str.lower()\n",
    "govt_measures_data = govt_measures_data.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Categorical Columns\n",
    "\n",
    "govt_measures_data = govt_measures_data.reset_index()\n",
    "govt_measures_data['val'] = 1\n",
    "govt_measures_data = govt_measures_data.set_index(['index','country','measure']).unstack(level=2).fillna(0).groupby('country').max()\n",
    "govt_measures_data = govt_measures_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming Columns\n",
    "\n",
    "names = govt_measures_data.columns\n",
    "new_names = ['Country_Region', 'additional health/documents requirements upon arrival', 'amendments to funeral and burial regulations', 'awareness campaigns', 'border checks', 'border closure', 'changes in prison-related policies', 'checkpoints within the country', 'complete border closure', 'curfews', 'domestic travel restrictions', 'economic measures', 'emergency administrative structures activated or established', 'full lockdown', 'general recommendations', 'health screenings in airports and border crossings', 'humanitarian exemptions', 'international flights suspension', 'introduction of quarantine policies', 'limit product imports/exports', 'limit public gatherings', 'lockdown of refugee/idp camps or other minorities', 'mass population testing', 'military deployment', 'obligatory medical tests not related to covid-19', 'other public health measures enforced', 'partial lockdown', 'psychological assistance and medical social work', 'public services closure', 'requirement to wear protective gear in public', 'schools closure', 'state of emergency declared', 'strengthening the public health system', 'surveillance and monitoring', 'testing policy', 'visa restrictions']\n",
    "new_names = [\"gm_\"+ s for s in new_names]\n",
    "new_names[0] = 'Country_Region'\n",
    "govt_measures_data.columns = new_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join with the original dataset\n",
    "\n",
    "data = data.merge(govt_measures_data, how = 'left', on='Country_Region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Distance From China"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = lat_long[['country_region','lat','long']]\n",
    "lat_long = lat_long.dropna(0)\n",
    "\n",
    "#Wuhan Co-ordinates\n",
    "Wuhan_Cord = (30.583332, 114.2833330)\n",
    "\n",
    "#Calculate Distance from China\n",
    "def calc_distance(row, site_coords):\n",
    "    target_coords = (row['lat'], row['long'])\n",
    "    dist = geodesic(site_coords, target_coords).miles\n",
    "    return(dist)\n",
    "\n",
    "lat_long['distance_from_china'] = lat_long.apply(calc_distance, site_coords=Wuhan_Cord, axis=1)\n",
    "\n",
    "#Get Rid of Lat, Long Columns\n",
    "lat_long = lat_long.rename(columns={\"country_region\": \"Country_Region\"}).drop(['lat', 'long'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(lat_long, how = 'left', on='Country_Region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add CoVID Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Indicator Columns\n",
    "covid_indicators_data = covid_indicators_data.drop(['iso3'], axis=1)\n",
    "names2 = covid_indicators_data.columns\n",
    "names2 = [\"ci_\"+ s for s in names2]\n",
    "names2[0] = 'Country_Region'\n",
    "covid_indicators_data.columns = names2\n",
    "covid_indicators_data = covid_indicators_data.replace({'No data': 0.0001, 'x':0.0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with Existing Data\n",
    "data = data.merge(covid_indicators_data, how = 'left', on='Country_Region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to deal with overfitting/incorrect predictions later on \n",
    "def data_prep(data):\n",
    "    data = data.astype('float32')\n",
    "    data = np.nan_to_num(data)\n",
    "    \n",
    "    pt = PowerTransformer()\n",
    "    pt.fit_transform(data)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(data)\n",
    "\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp_mean.fit_transform(data)\n",
    "\n",
    "    sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "    sel.fit_transform(data)\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train and test \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2863: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:121: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2863: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:121: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2863: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:121: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    }
   ],
   "source": [
    "#Isolate Training and Testing Test\n",
    "\n",
    "#Training Set\n",
    "train = data[data['data_set'] == 'Train']\n",
    "train = train.drop(['data_set', 'Id', 'Province_State', 'Country_Region', 'Date'], axis = 1)\n",
    "\n",
    "train_confirmed_X = train.drop(['Fatalities', 'ConfirmedCases'], axis = 1)\n",
    "train_fatalities_X = train.drop(['Fatalities','ConfirmedCases'], axis = 1)\n",
    "\n",
    "train_confirmed_y = train['ConfirmedCases']\n",
    "train_fatalities_y = train['Fatalities']\n",
    "\n",
    "train_confirmed_X = data_prep(train_confirmed_X)\n",
    "train_fatalities_X = data_prep(train_fatalities_X)\n",
    "\n",
    "#Testing Set\n",
    "test = data[data['data_set'] == 'Test']\n",
    "test = test.drop(['data_set', 'Id', 'Province_State', 'Country_Region', 'Date','ConfirmedCases', 'Fatalities'], axis = 1)\n",
    "\n",
    "test = data_prep(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Evaluation Metric and Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSLE: Evaluation Metric\n",
    "\n",
    "def rmsle(real, predicted):\n",
    "    sum=0.0\n",
    "    for x in range(len(predicted)):\n",
    "        if predicted[x]<0 or real[x]<0: #check for negative values\n",
    "            continue\n",
    "        p = np.log(predicted[x]+1)\n",
    "        r = np.log(real[x]+1)\n",
    "        sum = sum + (p - r)**2\n",
    "    return (sum/len(predicted))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for making predictions\n",
    "\n",
    "def make_pred(train_x, train_y, model):\n",
    "    #Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.3, random_state=66)\n",
    "\n",
    "    # Create linear regression object\n",
    "    regr = model\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    #Transform to log scale\n",
    "    y_pred = np.exp(y_pred)-1\n",
    "    \n",
    "    y_test = np.array(y_test).astype(float)\n",
    "    y_test = np.exp(y_test)-1\n",
    "    \n",
    "    #Calculate Error\n",
    "    err = rmsle(y_test, y_pred)\n",
    "    \n",
    "    return(y_pred, err, regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmed Cases Prediction\n",
    "lin_pred_confirmed = make_pred(train_confirmed_X, train_confirmed_y,linear_model.LinearRegression())\n",
    "\n",
    "# Fatalities Cases Prediction\n",
    "lin_pred_fatalities = make_pred(train_fatalities_X, train_fatalities_y,linear_model.LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.328103957784514"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean RMSLE\n",
    "\n",
    "(lin_pred_confirmed[1]+lin_pred_fatalities[1])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmed Cases Prediction\n",
    "lasso_pred_confirmed = make_pred(train_confirmed_X, train_confirmed_y,linear_model.Lasso(alpha=0.1))\n",
    "\n",
    "# Fatalities Cases Prediction\n",
    "lasso_pred_fatalities = make_pred(train_fatalities_X, train_fatalities_y,linear_model.Lasso(alpha=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.419285223064493"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean RMSLE\n",
    "(lasso_pred_confirmed[1]+lasso_pred_fatalities[1])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=7.32834e-12): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=7.32834e-12): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "# Confirmed Cases Prediction\n",
    "ridge_pred_confirmed = make_pred(train_confirmed_X, train_confirmed_y,Ridge(alpha=1.0))\n",
    "\n",
    "# Fatalities Cases Prediction\n",
    "ridge_pred_fatalities = make_pred(train_fatalities_X, train_fatalities_y,Ridge(alpha=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.646777130221392"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean RMSLE\n",
    "(ridge_pred_confirmed[1]+ridge_pred_confirmed[1])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmed Cases Prediction\n",
    "tree_pred_confirmed = make_pred(train_confirmed_X, train_confirmed_y,tree.DecisionTreeRegressor())\n",
    "\n",
    "# Fatalities Cases Prediction\n",
    "tree_pred_fatalities = make_pred(train_fatalities_X, train_fatalities_y,tree.DecisionTreeRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9104719298477852"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean RMSLE\n",
    "(tree_pred_confirmed[1]+tree_pred_fatalities[1])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\vivek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Confirmed Cases Prediction\n",
    "forests_pred_confirmed = make_pred(train_confirmed_X, train_confirmed_y,RandomForestRegressor(max_depth=2, random_state=66, max_features = 'sqrt'))\n",
    "\n",
    "# Fatalities Cases Prediction\n",
    "forests_pred_fatalities = make_pred(train_fatalities_X, train_fatalities_y,RandomForestRegressor(max_depth=2, random_state=66, max_features = 'sqrt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.683731318178617"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean RMSLE\n",
    "(forests_pred_confirmed[1]+forests_pred_fatalities[1])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "\n",
    "# Confirmed Cases Prediction\n",
    "gb_pred_confirmed = make_pred(train_confirmed_X, train_confirmed_y,ensemble.GradientBoostingRegressor(**params))\n",
    "\n",
    "# Fatalities Cases Prediction\n",
    "gb_pred_fatalities = make_pred(train_fatalities_X, train_fatalities_y,ensemble.GradientBoostingRegressor(**params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9291424684558763"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean RMSLE\n",
    "(gb_pred_confirmed[1]+gb_pred_fatalities[1])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Decision Trees performed the best in terms of RMSLE, we choose decision tree model to make predictions on the test set and submit it on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirmed Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the testing set\n",
    "y_confirmedcases = tree_pred_confirmed[2].predict(test)\n",
    "y_confirmedcases = np.exp(y_confirmedcases)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fatalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the testing set\n",
    "y_fatalities = tree_pred_fatalities[2].predict(test)\n",
    "y_fatalities = np.exp(y_fatalities)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series(np.arange(1,len(y_fatalities)+1).astype(int))\n",
    "b = pd.Series(y_confirmedcases)\n",
    "c = pd.Series(y_fatalities)\n",
    "\n",
    "result = pd.concat([a, b, c], axis=1, sort=False)\n",
    "result.columns = ['ForecastId','ConfirmedCases', 'Fatalities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('submission_vs.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The test RMSLE is 1.23572, which means we are in the top 200 submissions. These are vanilla models but we have a lot of scope to use hyper-parameter tuning and other complex techniques to make better predictions.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
